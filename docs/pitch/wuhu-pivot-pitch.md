---
marp: true
theme: default
paginate: true
backgroundColor: #fff
style: |
  section {
    font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Display', 'Helvetica Neue', sans-serif;
  }
  section.lead h1 {
    font-size: 2.8em;
    color: #e67e22;
  }
  section.lead h2 {
    font-size: 1.4em;
    color: #666;
    font-weight: 400;
  }
  h1 { color: #e67e22; }
  h2 { color: #333; }
  strong { color: #e67e22; }
  code { background: #f5f0eb; color: #e67e22; }
  table { font-size: 0.85em; }
  blockquote { border-left: 4px solid #e67e22; padding-left: 1em; color: #666; }
---

<!-- _class: lead -->

# ü¶û Wuhu

## AI-Native Team Workspace
## Built in 5 Days. By One Person. With AI Agents.

---

# What is Wuhu?

An **all-in-one workspace** where humans and AI agents collaborate as peers.

Not another coding agent ‚Äî the **workspace itself**.

| Replace | With Wuhu |
|---------|-----------|
| Notion / Obsidian | Workspace engine (markdown + live queries) |
| Linear | Issues (frontmatter + kanban) |
| Slack / Lark | Channels (humans + agents together) |
| GitHub (someday) | Agent-managed merge queue |

**Self-hostable.** Your data, your server, your agents.

---

# What Exists Today

Built in **5 working days** (the other 5 were Êò•ËäÇ üßß):

- ‚úÖ Full coding agent: bash, file ops, async tasks, compaction
- ‚úÖ Server + CLI + native macOS app
- ‚úÖ Real-time streaming (SSE subscriptions)
- ‚úÖ Channels with agent participation
- ‚úÖ Issues kanban from markdown files
- ‚úÖ Workspace docs with YAML frontmatter
- ‚úÖ Self-fork: agents spawning sub-agents
- ‚úÖ Crash-resilient: restart process ‚Üí agents resume

> Yesterday I shipped 4-5 features **using Wuhu to build Wuhu**.

---

# Architecture: Swift + Actors + SQLite

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Wuhu Server              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Session  ‚îÇ  ‚îÇ   Session    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Runtime  ‚îÇ  ‚îÇ   Runtime    ‚îÇ  ‚îÇ  ‚Üê Swift actors (in-memory state)
‚îÇ  ‚îÇ (actor)  ‚îÇ  ‚îÇ   (actor)    ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ       ‚îÇ               ‚îÇ          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ      SQLite (GRDB/WAL)     ‚îÇ  ‚îÇ  ‚Üê Single file, append-only
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ       ‚îÇ               ‚îÇ          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ OpenAI  ‚îÇ    ‚îÇ Anthropic  ‚îÇ  ‚îÇ  ‚Üê LLM APIs
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ WebSocket
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Runner    ‚îÇ  ‚Üê Stateless, any machine
    ‚îÇ (bash/files)‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

# Why This Architecture?

**Single process. Single database. In-memory actors.**

- Cloud-native coding agents read full transcript from DB on **every** tool result ‚Üí reconstruct prompt ‚Üí call LLM ‚Üí discard. That's insane.
- Wuhu: transcript lives in memory. DB is for durability. Hot loop is **O(1)**.
- No Kafka, no Redis, no Kubernetes. Runs on a **$10 VPS**.

**Crash resilience** ‚Äî persist first, then update memory.
If the process crashes mid-session, restart ‚Üí agent loop picks up exactly where it left off. Stale tool calls get auto-repaired.

**Scaling path** ‚Äî SQLite shards (sessions are independent), WAL streaming to replicas. Turso/libSQL may solve multi-writer before we need it.

---

# Server + Runner: Brain vs Hands

The server is the **brain** ‚Äî sessions, LLM calls, decisions.
Runners are the **hands** ‚Äî execute bash, read/write files.

**One agent, multiple computers:**

```
Agent Loop (server)
  ‚îú‚îÄ‚îÄ "read frontend repo"    ‚Üí Runner A (MacBook)
  ‚îú‚îÄ‚îÄ "run backend tests"     ‚Üí Runner B (VPS)
  ‚îî‚îÄ‚îÄ "deploy to staging"     ‚Üí Runner C (Lambda)
```

Runner protocol is language-agnostic (JSON over WebSocket).
Implement in **Swift, TypeScript, Python** ‚Äî whatever fits the infra.

Scale out compute. Keep decisions centralized.

---

# The Session Model

**Append-only transcript** + **three input lanes**:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Agent Loop              ‚îÇ
‚îÇ                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                        ‚îÇ
‚îÇ  ‚îÇ System   ‚îÇ ‚Üê async_bash callback  ‚îÇ  Interrupt
‚îÇ  ‚îÇ Lane     ‚îÇ   task notifications   ‚îÇ  checkpoint
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                        ‚îÇ  (between
‚îÇ  ‚îÇ Steer    ‚îÇ ‚Üê "stop, do X instead" ‚îÇ   tool calls)
‚îÇ  ‚îÇ Lane     ‚îÇ   urgent corrections   ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                        ‚îÇ
‚îÇ  ‚îÇ Follow-up‚îÇ ‚Üê "now do Y"           ‚îÇ  Turn
‚îÇ  ‚îÇ Lane     ‚îÇ   next-turn input      ‚îÇ  boundary
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îÇ
‚îÇ                                      ‚îÇ
‚îÇ  Transcript: [entry‚ÇÅ, entry‚ÇÇ, ...]  ‚îÇ
‚îÇ  Compaction: sum‚ÇÅ ‚Üí sum‚ÇÇ ‚Üí kept     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Steer** = course-correct a running agent without waiting.
**Follow-up** = queue work for when it finishes.

---

# Self-Fork: Agents Spawning Agents

A channel agent chats with you. You say "fix that bug."

```
You:        "Fix the auth bug in issue #0031"
                    ‚îÇ
            Channel Agent (parent)
            ‚îú‚îÄ‚îÄ fork("Fix auth bug", env: backend)
            ‚îú‚îÄ‚îÄ "On it! I've started a coding session."
            ‚îÇ        ‚îÇ
            ‚îÇ   Coding Agent (child)
            ‚îÇ   ‚îú‚îÄ‚îÄ reads issue
            ‚îÇ   ‚îú‚îÄ‚îÄ edits files
            ‚îÇ   ‚îú‚îÄ‚îÄ runs tests
            ‚îÇ   ‚îî‚îÄ‚îÄ ‚úÖ done ‚Üí notifies parent
            ‚îÇ        ‚îÇ
            ‚îú‚îÄ‚îÄ "Auth bug is fixed. PR ready."
            ‚îÇ
You:        "Great, how about the frontend?"
```

Parent stays **responsive**. Child works with **full context**.
Async notification on completion (like `async_bash`).

---

# The `wuhu` Tool: One Tool to Rule Them All

**Problem:** Rich environment = many tools = bloated LLM context.

**Solution:** Embed **QuickJS** in the server. One tool, takes JS:

```javascript
wuhu(`
  const issues = workspace.query({
    kind: 'issue', status: 'open', assignee: 'minsheng'
  });
  const urgent = issues.filter(i => i.priority === 'critical');
  sessions.create({
    env: 'backend',
    title: 'Fix ' + urgent[0].title
  });
  channels.post('engineering',
    'Starting work on ' + urgent[0].title);
`)
```

**One tool call. Multiple operations. Zero round trips.**

MCP tools auto-discovered as JS functions. `wuhu('help')` for docs.

---

# Event-Driven Async Agents

What if an agent is a **Telegram bot**? User messages = steer notifications:

```
xxx ‚Üê tool result

Minsheng <DM> 15:34
Oh by the way create a PR after you are done!

Yihan <DM> 15:35
How are we on the current milestone?
```

Agent responds with tool calls:

```
send_message(target: "Minsheng", content: "Gotcha!")
send_message(target: "Yihan", content: "Almost done!")
```

**One agent. Multiple conversations. Real-time async feel.**

The infrastructure already supports this (steer queue + tool calls).
The experiment: does the model generalize? **Let's find out.**

---

# Workspace: The Git-Native Knowledge Base

Every doc is a **markdown file** with YAML frontmatter. Git-managed.

**Implicit rules** (`wuhu.yml`):
```yaml
rules:
  - path: "issues/**"
    metadata: { kind: issue }
```

**Database views** (live queries in docs):
~~~
```database-view
id: my-open-issues
---
SELECT * FROM documents
WHERE kind = 'issue' AND status = 'open'
ORDER BY priority DESC
```
~~~

**Personalized home pages** ‚Äî your dashboard is a markdown file with your queries.

Agents read, write, and query the same docs. No vendor lock-in.

---

# Channels: Where Humans Meet Agents

Not replacing Lark ‚Äî **extending** it.

A channel where the agent is a participant:
- You say "fix that bug" ‚Üí agent responds, forks a session, reports back
- Agent posts intent before acting: "I'll update the schema"
- Multiple humans + agents in one channel

**Human-centric.** Agents read from and post to channels via tool calls.
Event-driven notifications keep agents informed.

What Lark would be if it was built for human-AI collaboration.

---

# Native Apps: The Differentiator

In a sea of **Electron apps** ‚Äî Codex, Claude Desktop, Cursor ‚Äî
a native SwiftUI app **stands out**.

- **macOS** ‚Äî already working, real-time streaming, kanban, channels
- **iOS** ‚Äî shared TCA reducers, adaptive UI (coming soon)
- **visionOS** ‚Äî name one AI coding tool with a visionOS app. Zero.

Shared codebase thanks to **TCA** (ComposableArchitecture by Point-Free).
Ultra-modularized: each feature is its own module, independently testable.

CLI for every interaction. You can use Wuhu entirely headless.
But you won't ‚Äî because the app is that good.

> HackerNews bait: "Native macOS/iOS/visionOS AI workspace"

---

# Observability: Know Everything

**Raw LLM logging** ‚Äî every request and response. Debug anything in seconds.

**Cost tracking:**
- Per session, per user, per issue resolved
- "How much did this feature cost in AI compute?"

**Task reproduction:**
- Replay historical tasks on new models
- "Can DeepSeek 4 solve this? How much cheaper?"
- Regression detection: "Did Anthropic nerf Claude?"

**Team metrics:**
- Who's productive with AI? Who's struggling? Why?
- Is it a setup issue? A prompting skill issue? Micro-managing?

---

# The Roadmap

| Week | Milestone |
|------|-----------|
| **Week 1** | Extract PiAI to own repo. Set up multi-repo workflow. |
| **Week 2** | Workspace query engine v0.1. Implicit rules + basic queries. |
| **Week 3** | Integrate workspace engine into Wuhu. iOS app prototype. |

**Ongoing:** Agent experimentation (multi-channel bot, async assistant).

**The pattern:** Small repos ‚Üí agents iterate fast ‚Üí pin stable versions ‚Üí integrate.

Each repo is a sub-team. The team lead is me. The engineers are coding agents.

---

<!-- _class: lead -->

# Wuhu

## Your workspace. Your server. Your agents.

## Built with AI. For teams that work with AI.

**5 days ‚Üí working product. Imagine 5 months.**

