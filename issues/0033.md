---
title: "Set default maxTokens to model's max output / 3 using a model spec catalog"
status: done
priority: high
---

## Problem

Wuhu currently does not set `maxTokens` in `RequestOptions` for agent inference calls. This causes:

- **Anthropic**: falls back to the hardcoded PiAI default of `1024` in `AnthropicMessagesProvider` — far too low for a coding agent, frequently hitting `stopReason: "length"` and producing truncated/empty tool calls (root cause of WUHU-0032).
- **OpenAI / OpenAI Codex**: doesn't send `max_output_tokens` at all, leaving it to the API's server-side default — unpredictable and likely not generous enough.

A coding agent should have generous output budgets. The upstream reference project (`pi-coding-agent`) uses **model max output tokens / 3** as the default, which is a sensible heuristic.

## Proposed Changes

### 1. Add a model spec table with input/output token limits

Add a `WuhuModelSpec` type (or extend `WuhuModelCatalog`) with hardcoded token limits for the frontier models we actually use:

**Anthropic** (all 200k input):
| Model | Max Output |
|---|---|
| claude-opus-4-5 | 128k |
| claude-opus-4-6 | 128k |
| claude-sonnet-4-5 | 64k |
| claude-sonnet-4-6 | 64k |
| claude-haiku-4-5 | 64k |

**OpenAI** (all 400k input, 128k output):
| Model | Notes |
|---|---|
| gpt-5 | |
| gpt-5.1 | |
| gpt-5.2 | |
| gpt-5-codex | |
| gpt-5.1-codex | |
| gpt-5.2-codex | |

### 2. Clean up the model catalog

- **Anthropic**: Remove `claude-haiku-4-6` (only keep haiku 4.5). Keep opus and sonnet for both 4.5 and 4.6.
- **OpenAI**: Remove mini/nano/max variants (`gpt-5.1-codex-mini`, `gpt-5.1-codex-max`). Keep the six models listed above.

### 3. Default maxTokens = maxOutputTokens / 3

In `makeRequestOptions` (or wherever the inference request is built), look up the model's max output tokens from the spec table and set `maxTokens = maxOutput / 3` as the default. This gives:

- Anthropic Opus: ~42k default
- Anthropic Sonnet/Haiku: ~21k default
- OpenAI models: ~42k default

If a model is not found in the spec table, fall back to a reasonable default (e.g. 16384).

### 4. Remove the Anthropic provider's 1024 fallback

Once `maxTokens` is always set by the caller, the `?? 1024` fallback in `AnthropicMessagesProvider` becomes dead code. Consider either raising it to a sane fallback or documenting it as a safety net.

## Acceptance Criteria

- [ ] `WuhuModelCatalog` (or a new `WuhuModelSpec` type) contains hardcoded input/output token limits for the listed models
- [ ] `makeRequestOptions` computes `maxTokens` as `maxOutput / 3` from the spec table
- [ ] Old/unwanted models removed from the catalog (haiku 4.6, mini/max variants)
- [ ] Tests pass (`swift test`)
- [ ] The Anthropic 1024 fallback in PiAI is raised or documented as a safety net
