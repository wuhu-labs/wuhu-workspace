---
title: "Optimize DocC R2 upload in CI"
status: open
assignee: agent
priority: medium
---

# WUHU-0042: Optimize DocC R2 upload in CI

## Problem

The DocC deploy workflow for repos with many targets (especially wuhu-core
with 7 library targets) is extremely slow because files are uploaded to
Cloudflare R2 one at a time using `wrangler r2 object put` in a serial loop.

wuhu-core generates ~2000+ files across all targets. At ~1 second per PUT
call, the upload step alone takes 30+ minutes.

## Options

1. **Parallel uploads** — use `xargs -P` or GNU parallel to upload multiple
   files concurrently. Simple change, big speedup.
2. **S3-compatible API** — R2 supports S3, so `aws s3 sync --endpoint-url`
   with parallel transfer would be fast and handles retries natively.
3. **Reduce target count** — only generate docs for public-facing targets
   instead of all 7 (e.g., WuhuAPI, WuhuCore, WuhuClient only).
4. **Batch via tar/zip** — upload a single archive and use a Worker to
   unpack, though this adds complexity.

## Recommendation

Option 1 (parallel xargs) is the lowest effort. Option 2 (aws s3 sync) is
the most robust long-term.

## Affected repos

- `wuhu-core` (7 targets, worst case)
- `wuhu-workspace-engine` (3 targets, moderate)
- `wuhu-ai` (1 target, fine as-is)

## Context

Set up in the wuhu.ai website work session (2026-02-28). The current serial
upload approach works but is impractically slow for large repos.
