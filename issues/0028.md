---
title: "Implement prompt caching for Anthropic models"
status: done
priority: high
---

## Context

Anthropic's Messages API supports [prompt caching](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching)
via `cache_control` breakpoints on content blocks. This can dramatically reduce
latency and cost for multi-turn conversations and long system prompts by letting
the API reuse prefixes it has already processed.

The OpenAI providers (`OpenAICodexResponsesProvider`,
`OpenAIResponsesProvider`) already pass `prompt_cache_key` /
`prompt_cache_retention` where applicable. The Anthropic provider
(`AnthropicMessagesProvider`) currently sends no caching directives at all.

## What Needs to Change

### 1. Send the `anthropic-beta` header

Prompt caching requires the beta header:

```
anthropic-beta: prompt-caching-2024-07-31
```

Add this to the request headers in `AnthropicMessagesProvider.stream(...)`.

### 2. Add `cache_control` to the system prompt

When `context.systemPrompt` is set, the provider currently sends it as a plain
string:

```json
"system": "<text>"
```

Change this to the block form so a cache breakpoint can be attached:

```json
"system": [
  {
    "type": "text",
    "text": "<text>",
    "cache_control": { "type": "ephemeral" }
  }
]
```

### 3. Add `cache_control` to the last user turn

In `buildBody`, identify the final user message in the `params` array and add
`"cache_control": { "type": "ephemeral" }` to the last content block of that
message. This tells Anthropic to cache everything up to and including that
turn, so subsequent assistant round-trips benefit from prefix caching.

### 4. Surface cache performance in usage metadata (stretch)

Anthropic returns `cache_creation_input_tokens` and `cache_read_input_tokens`
in the `message_start` and `message_delta` events. If we already surface a
`Usage` or token-count struct on `AssistantMessage`, populate those fields so
callers can observe cache hit rates. If not, note this for a follow-up.

## Files to Study

- `Sources/PiAI/Providers/AnthropicMessagesProvider.swift` — primary change
  site (`buildBody`, request header setup)
- `Sources/PiAI/Providers/OpenAICodexResponsesProvider.swift` — reference for
  how OpenAI prompt caching is wired
- `Sources/PiAI/Providers/OpenAIResponsesProvider.swift` — same
- `Sources/PiAI/Models/` — `AssistantMessage`, `Context`, `RequestOptions` for
  any model changes needed

## Acceptance Criteria

- [ ] Anthropic requests include `anthropic-beta: prompt-caching-2024-07-31`
- [ ] System prompt is sent in block form with `cache_control`
- [ ] Last user turn includes a `cache_control` breakpoint
- [ ] Existing `AnthropicMessagesProviderTests` still pass
- [ ] New test(s) verify that the serialized request body contains the expected
      `cache_control` blocks
- [ ] (Stretch) Cache-hit token counts are surfaced if a usage model exists
